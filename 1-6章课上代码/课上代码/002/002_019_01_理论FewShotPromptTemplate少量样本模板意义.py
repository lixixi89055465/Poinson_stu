# FewShotPromptTemplate少量样本模板意义:
# llms,不能推理的模型,不举例子,他很难指导怎么给你答案,
# 核心概念
# 少样本学习：与传统的大量数据训练不同，少样本学习通过提供少量的示例，让模型快速学习并泛化到新的任务上。例如，你想让模型完成一个文本分类任务，只需要提供几个分类的示例，模型就能对新的文本进行分类。
#为什么要选择示例:动态示例的选择,比如我们这里有大量的实例,例如我们要做一个健身指导手册,里面有大量例子,但是总长度超过deepseek,上下文长度的64k,我们有1tb,太长,无法全部发送给大模型,只能选择其中一小部分例子,那么我们可以1.按长度选,2.按最大边际相关性选(相似度+多样性),3.按相似度选,4.按重叠度选.让大模型参考例子,生成更多的视频文案,其中健身指导手册正规的健美,古典,健体,力量举,孜然健身,真自然撸铁组,拍照组,和丧钟克里斯,喜钟酷尔曼,zhouzhou的气体加速系统
#大模型会对提示词长度进行控制,主要原因:
#例如deepseek api开发平台官网:
# https://api-docs.deepseek.com/zh-cn/quick_start/pricing/
#明确写出他的2个模型是64k的tokens长度,那么超出这个长度就不行,所以需要截取示例,
# 上下文窗口大小是自然语言处理（NLP）尤其是大型语言模型（LLM）里的一个关键概念，它指的是模型在一次处理中能够接收和处理的输入文本的最大长度，一般用标记（token）数量来衡量。下面从几个方面详细解释：

# 定义和表现形式
# Token 层面：在 NLP 里，文本通常会被分割成更小的单元，也就是标记（token）。这些标记可以是单词、子词或者字符。上下文窗口大小规定了模型一次能处理的最大标记数量。例如，GPT - 3.5 的上下文窗口大小通常是 4096 个标记，这意味着输入文本（包含提示词和历史对话等）转化为标记后，总数不能超过 4096 个。
# 实际影响：若输入文本的标记数量超出了上下文窗口大小，就会出现问题。一些模型会直接截断超出部分的文本，导致信息丢失；还有些模型可能会报错，无法处理输入。

#过程
# 其中除了长度和重叠度不需要 训练模型和嵌入向量数据库,只需要进行简单计算就能选出被筛选出的示例
# MMR最大边际相关性和相速度,这2个因为需要把自然语言转换成可以理解的数据计算,需要把示例放到训练模型中,再嵌入到向量数据库中,再设置选择数量,再筛选出模型
#MMR比相似度的,算法多了一个多样性,他俩共同有的是相似性.